<pre class='metadata'>
Title: Completing the Design of Executors
Shortname: D2300
Revision: 0
Status: D
Group: WG21
Audience: SG1, LEWG
Editor: Michał Dominiak, NVIDIA, griwes@griwes.info
Editor: Bryce Adelstein Lelbach, NVIDIA, brycelelblach@gmail.com
URL: https://wg21.link/D2300
Metadata Order: Editor, This Version, Source, Issue Tracking, Project, Audience
Markup Shorthands: markdown yes
Toggle Diffs: no
No Abstract: yes
</pre>

<style>
pre {
  margin-top: 0px;
  margin-bottom: 0px;
}
.ins, ins, ins *, span.ins, span.ins * {
  background-color: rgb(200, 250, 200);
  color: rgb(0, 136, 0);
  text-decoration: none;
}
.del, del, del *, span.del, span.del * {
  background-color: rgb(250, 200, 200);
  color: rgb(255, 0, 0);
  text-decoration: line-through;
  text-decoration-color: rgb(255, 0, 0);
}
math, span.math {
  font-family: serif;
  font-style: italic;
}
ul {
  list-style-type: "— ";
}
blockquote {
  counter-reset: paragraph;
}
div.numbered, div.newnumbered {
  margin-left: 2em;
  margin-top: 1em;
  margin-bottom: 1em;
}
div.numbered:before, div.newnumbered:before {
  position: absolute;
  margin-left: -2em;
  display-style: block;
}
div.numbered:before {
  content: counter(paragraph);
  counter-increment: paragraph;
}
div.newnumbered:before {
  content: "�";
}
div.numbered ul, div.newnumbered ul {
  counter-reset: list_item;
}
div.numbered li, div.newnumbered li {
  margin-left: 3em;
}
div.numbered li:before, div.newnumbered li:before {
  position: absolute;
  margin-left: -4.8em;
  display-style: block;
}
div.numbered li:before {
  content: "(" counter(paragraph) "." counter(list_item) ")";
  counter-increment: list_item;
}
div.newnumbered li:before {
  content: "(�." counter(list_item) ")";
  counter-increment: list_item;
}
</style>

# Abstract # {#abstract}

This paper contains a self-contained design for an executors library for C++, based on the ideas in [[P0443R14]] and its companion papers.

## Code example ## {#design-code}

Using the facilities in this paper, an end user can write code such as this:

<pre highlight="c++">
using namespace std::execution;

// get a scheduler from somewhere, e.g. a thread pool
executor auto sch = get_thread_pool().scheduler();

// describe a chain of dependent work
sender auto begin = schedule(sch);
sender auto hi_again = then(begin, []{
    std::cout << "Hi again! Have an int.";
    return 13;
});
sender auto work = then(hi_again, [](int arg) { return arg + 42; });

// submit the work for execution on the pool
// block the current thread until its completion
// the return value is a tuple of values being the result of the sender chain
auto [i] = std::this_thread::sync_wait(work);
</pre>

See [[#design-factories]], [[#design-adapters]], and [[#design-algorithms]] for short explanations of the algorithms used in this code example.

## What this proposal is ## {#abstract-is}

This paper describes a design for a complete, self-contained library providing precise control for code execution on varying execution contexts in C++. This paper should be considered as a full design on its own, based heavily on prior papers that have been seen and
discussed by the committee.

Together with the paper, we are presenting a full, standalone, implementation of the design detailed in this paper, one that should satisfy the requirements of the prior papers that have been seen by the committee, but one that has also been validated to satisfy a
good part of the requirements of accelerator runtimes such as CUDA; our work to provide an efficient CUDA implementation of the sender/receiver model has directly driven most of the novel changes proposed here.

We are aiming at satisfying all the promises that have been given when the sender/receiver model has been proposed and adopted by SG1, most importantly that:

1. sender algorithms will allow for customization of their behavior based on the scheduler they are invoked for; and
2. the sender/receiver model is a replacement for two-way executors, which provided precise controls over places of execution of user code.

We believe that this aim has been achieved by the design proposed here.

## What this proposal is **not** ## {#abstract-is-not}

This paper is not a patch on top of [[P0443R14]]; we are not asking to update the existing paper, we are asking to retire it in favor of this paper, which is already self-contained; any example code within this paper can be written in Standard C++, without the need
to standardize any further facilities.

This paper is not an alternative design to [[P0443R14]]; rather, we have taken the design in the current executors paper, and applied targetted fixes to allow it to fulfill the promises of the sender/receiver model, as well as provide all the facilities we consider
essential when writing user code using standard execution concepts; we have also applied the guidance of removing one-way executors from the paper entirely, and instead provided an algorithm based around senders that serves the same purpose.

## What are the major design changes compared to P0443? ## {#abstract-compare}

1. This proposal does not propose any specific type erasure facilities; it does, however, discuss type erasure to an extent in [[#design-dispatch]].
2. Properties are not included in this paper; we see them as a possible future extension, if the committee gets more comfortable with them.
3. This paper does not include a specific thread pool implementation, per prior committee direction to propose it separately.
4. We have implemented the SG1 direction to remove executors and base all of the proposed functionalities on senders and schedulers.
5. Senders now advertise what scheduler, if any, they are bound to.
6. Users now have a choice between using a strictly lazy vs a possibly eager version of most algorithms.
7. The places of execution of user code in P0443 weren't precisely defined, whereas they are in this paper. See [[#design-propagation]].
8. P0443 did not propose a suite of algorithms necessary for writing sender code; this paper does. See [[#design-factories]], [[#design-adapters]], and [[#design-algorithms]].
9. P0443 did not specify the semantics of variously qualified `connect` overloads; this paper does. See [[#design-fork]].

# Revision history # {#revisions}

## R0 ## {#r0}

Initial revision, still in progress.

# Proposed design - introduction # {#design-intro}

The following four sections describe the entirety of the proposed design. [[#design-intro]] describes the conventions used through the rest of the design sections, as well as an example illustrating how we envision code will be written using this proposal.

[[#design-user]] describes all the functionality from the perspective we intend for users: it describes the various concepts they will interact with, and what their programming model is. [[#design-implementer]] describes the machinery that allows for
that programming model to function, and the information contained there is necessary for people implementing senders and sender algorithms (including the standard library ones) - but is not necessary for being able to use senders productively.

Finally, [[#design-api]] discusses API decisions and questions that we believe reflect on the code that will need to be written to implement and use senders, but not on the programming model of senders in general.

Note: none of the algorithm names proposed here are names that we are particularly attached to; consider the names of the algorithms to be reasonable placeholders that can freely be changed, should the committee want to do so.

## Conventions ## {#design-conventions}

The following conventions are used throughout the design section:

  1. Universal references and explicit calls to `std::move`/`std::forward` are omitted in code samples and signatures for simplicity; assume universal references and perfect forwarding unless stated otherwise.
  2. The namespace proposed in this paper is the same as in [[P0443R14]]: `std::execution`; however, for brevity, the `std::` part of this name is omitted. When you see `execution::foo`, treat that as `std::execution::foo`.

# Proposed design - user side # {#design-user}

## Execution contexts describe the place of execution ## {#design-contexts}

An <dfn>execution context</dfn> is a resource that represents the *place* where execution will happen. This could be a concrete resource - like a specific thread pool object, or a GPU - or a more abstract one, like the current thread of execution. Execution contexts
don't need to have a representation in code; they are simply a term describing certain properties of execution of a function.

## Schedulers represent execution contexts ## {#design-schedulers}

A <dfn>scheduler</dfn> is a lightweight handle to an *execution context*, which allows describing work that will be executed on an execution agent belonging to that context. Since execution contexts don't necessarily manifest in C++ code, it's not possible to program
directly against their API. A scheduler is a solution to that problem: the scheduler concept is defined by a single operation, `schedule`, which creates a description of work on its associated execution context, materialized in the form of a sender.

<pre highlight="c++">
execution::scheduler auto sch = get_thread_pool().scheduler();
execution::sender auto snd = execution::schedule(sch);
// snd is a sender (see below) describing the creation of a new execution resource
// on the execution context associated with sch
</pre>

## Senders describe work ## {#design-senders}

A <dfn>sender</dfn> is an object that describes work. Senders are similar to futures in existing asynchrony designs, but unlike futures, the work that is being done to arrive at the values they will *send* is also directly described by the sender object itself. A
sender is said to <dfn>send</dfn> some values if a receiver connected (see [[#design-connect]]) to that sender will eventually *receive* said values.

The primary defining operation of senders is [[#design-connect]]; this function, however, is not a user-facing API; it is used to facilitate communication between senders and various sender algorithms (or other senders), but end user code is not expected to invoke
it directly. The way user code is expected to interact with senders is by using said sender algorithms; the opening example of the design section already contains a sample of sender algorithms, and the ones we are proposing as a part of this paper are described in
[[#design-factories]], [[#design-adapters]], and [[#design-algorithms]]. Here is how a user can ensure that work is started (and will eventually complete if no errors happen), but without waiting for it to finish:

<pre highlight="c++">
execution::scheduler auto sch = get_thread_pool().scheduler();
execution::sender auto snd = execution::schedule(sch);
execution::sender auto cont = execution::then(snd, []{
    std::fstream file{ "result.txt" };
    file << compute_result;
});

execution::submit(cont);
// at this point, the work described by \`cont\` has been submitted to the thread pool
</pre>

## Senders may be bound to schedulers ## {#design-propagation}

One of the less clear aspects of [[P0443R14]] has been the set of requirements for the <i>place of execution</i> of any given piece of code. This underspecification is problematic, especially in case of systems where not all execution agents are created equal,
and not all functions can be run on all execution agents. Having precise control over the execution context used for any given function call being submitted is very important on such systems, and the users of standard execution facilities will expect to be able
to express such (binding) requirements.

Such precise control was present in the two-way execution API, but it has so far been missing from the senders design. There has been a proposal ([[P1897R3]]) to provide a number of algorithms that would enforce certain rules on the places of execution
of the work described by a sender, but we have found those algorithms to be insufficient for achieving the best performance on all platforms that are of interest to us. The implementation strategies that we are aware of result in one of the following situations:

  1. trying to submit work to CPU execution contexts (e.g. a thread pool) from an accelerator (e.g. a GPU), which assumes that the accelerator threads of execution are as capable as the CPU threads of execution (which they aren't); or
  2. forcibly interleaving two adjacent execution graph nodes that are both executing on an accelerator with glue code that runs on the CPU; this operation is prohibitively expensive on runtimes such as CUDA.

Neither of these implementation strategies is acceptable for accelerator runtimes. Therefore, in addition to the `on` algorithm from [[P1897R3]], we are proposing to add a standardized way for senders to advertise what scheduler (and by extension - what execution
context) any work (save for explicit transitions) attached to them will execute on. Any given sender <b>may</b> have an <dfn>completion scheduler</dfn>. When further work is attached to that sender by invoking sender algorithms, that work will also execute on the
completion scheduler.

There exists a function to retrieve the [=completion scheduler=] from a sender, called `get_completion_scheduler`. Calling this function on a sender that does not have an completion scheduler is ill-formed. If a scheduler advertises its completion scheduler in this
way, that sender <b>must</b> ensure that it [=send|sends=] its values on an execution agent belonging to an execution context represented by a scheduler returned from this function.

<pre highlight="c++">
execution::scheduler auto sch = new_thread_scheduler{};
// sch is a scheduler that starts work on a new thread

execution::sender auto initial = execution::schedule(sch);
execution::sender auto next = execution::then(initial, []{
    std::cout << "First continuation" << std::endl;
});
execution::sender auto last = execution::then(next, []{
    std::cout << "Second continuation" << std::endl;
});

execution::scheduler auto completion_sch = execution::get_completion_scheduler(last);
// completion_sch is equivalent to sch

std::this_thread::sync_wait(last);
// both continuations will run on the same thread created by the scheduler
</pre>

## Execution context transitions are explicit ## {#design-transitions}

[[P0443R14]] does not contain any mechanisms for performing an execution context transition. The only algorithm that can create a sender that will move execution to a specific* execution context is `execution::schedule`, which does not take a predecessor, which
means that there's no way to construct sender chains that traverse different execution contexts. This is necessary to fulfill the promise of senders being able to replace two-way executors, which had this capability.

We propose that, for senders advertising their [=completion scheduler=], all execution context transitions <b>must</b> be explicit; running user code anywhere but where they defined it to run <b>must</b> be considered a bug.

We propose a new user-facing algorithm, `execution::reschedule`, for performing transitions from one execution context to another:

<pre highlight="c++">
execution::scheduler auto sch1 = ...;
execution::scheduler auto sch2 = ...;

execution::sender auto snd1 = execution::schedule(sch1);
execution::sender auto then1 = execution::then(snd1, []{
    std::cout << "I am running on sch1!\n";
});

execution::sender auto snd2 = execution::reschedule(then1, sch2);

execution::sender auto then2 = execution::then(snd2, []{
    std::cout << "I am running on sch2!\n";
});

std::this_thread::sync_wait(then2);
</pre>

## Senders are forkable ## {#design-fork}

Any non-trivial program will eventually want to fork a chain of senders into independent streams of work; an incoming event to a middleware system may be required to trigger events on more than one downstream system, for instance. This requires that senders provide
well defined mechanisms for making sure that attaching multiple continuations to a sender is possible and correct.

There are two parts of what we are proposing in this area. One is a semantic requirement on both senders and sender algorithms: if you invoke a sender algorithm with an lvalue sender, that sender must remain valid after such an algorithm invocation. If your algorithm
accepts an lvalue sender, it must not invalidate the lvalue it has received. It must be possible to invoke sender algorithms with rvalue senders, but lvalues must not be invalidated other than by turning them into rvalues ("moving" them).

The consequences of the above requirement are as follows:

 * single-shot senders should only work with rvalue-qualified overloads of sender algorithms;
 * multi-shot senders should work with at least some lvalue-qualified algorithms; and
 * multi-shot senders can still have operations that require moving them into an algorithm call.

To facilitate attaching multiple continuations to the same sender multiple times regardless of whether it is single-shot or multi-shot, we are also proposing a new sender algorithm: `split`.

<pre highlight=c++>
auto some_algorithm(execution::sender auto predecessor) {
    // here, we can test whether the predecessor is multi-shot
    // (for the purposes of the algorithms being used in this function)

    // but we can also turn the predecessor into a multi-shot sender:

    execution::sender auto multi_shot = split(predecessor);
    // multi_shot is guaranteed to be multi-shot
}
</pre>

## Senders are joinable ## {#design-join}

Similarly to how it's hard to write a complex program that will eventually want to fork sender chains into independent streams, it's also hard to write a program that does not want to eventually create join nodes, where multiple independent streams of execution are
merged into a single one in an asynchronous fashion. We are proposing an algorithm that deals with the common use case for such merging: `when_all`, and an additional variant, `reschedule_when_all`, which takes an additional scheduler.

The sender returned from `when_all` completes when the last of the predecessor senders completes. It [=send|sends=] a pack of values, where the elements of said pack are the values sent by the predecessor senders, in order. The `when_all` algorithm returns a sender
that also does not have an associated scheduler.

`reschedule_when_all` accepts an additional scheduler argument. It returns a sender whose [=completion scheduler=] is the scheduler provided as an argument, but otherwise behaves the same as `when_all`. You can think of it as a composition of
`transform(when_all(predecessors...), scheduler)`, but one that allows for better efficiency through customization.

## Proposed set of user-facing sender factories ## {#design-factories}

A <dfn>sender factory</dfn> is a function which creates new senders, without requiring a predecessor sender.

### `execution::schedule` ### {#design-factory-schedule}

<pre highlight="c++">
execution::sender auto schedule(
    execution::scheduler auto scheduler
);
</pre>

Returns a sender describing the start of a task graph on the provided scheduler. See [[#design-schedulers]].

<pre highlight="c++">
execution::scheduler auto sch1 = get_system_thread_pool().scheduler();

execution::sender auto snd1 = execution::schedule(sch1);
// snd1 describes the creation of a new task on the system thread pool
</pre>

### `execution::just` ### {#design-factory-just}

<pre highlight="c++">
execution::sender auto just(
    auto ...values
);
</pre>

Returns a sender with no [=completion scheduler=], which [=send|sends=] copies of the provided values.

TODO: example

### `execution::reschedule_just` ### {#design-factory-reschedule_just}

<pre highlight="c++">
execution::sender auto reschedule_just(
    execution::scheduler auto scheduler,
    auto ...values
);
</pre>

Returns a sender whose [=completion scheduler=] is the provided scheduler, which [=send|sends=] copies of the provided values.

<pre highlight="c++">
execution::sender auto vals = execution::reschedule_just(
    get_system_thread_pool().scheduler(),
    1, 2, 3
);
execution::sender auto snd = execution::then(pred, [](auto... args) {
    std::print(args..);
});
// when snd is executed, it will print "123"
</pre>

This algorithm is included, as it greatly simplifies lifting values into senders.

## Proposed set of user-facing sender adapters ## {#design-adapters}

A <dfn>sender adapter</dfn> is a function which accepts one or more senders, and possibly other algorithms, and returns a sender, whose completion is related to the sender arguments it has received.

Many sender adapters come in two versions: a strictly lazy one, which is never allowed to submit any work for execution prior to the returned sender being <dfn lt="start">started</dfn> later on, and a potentially eager one, which is allowed to submit work prior to
the returned sender being started. Sender algorithms such as [[#design-adapter-ensure_started]], [[#design-algorithm-submit]], and [[#design-algorithm-sync_wait]] start senders; the implementations of non-lazy versions of the sender adapters are **allowed**, but not
guaranteed, to start senders.

The strictly lazy versions of the adapters below (that is, all the versions whose names start with `lazy_`) are guaranteed to not start any predecessor senders passed into them.

For more implementer-centric description of starting senders, see [[#design-laziness]].

### `execution::reschedule` ### {#design-adapter-reschedule}

<pre highlight="c++">
execution::sender auto reschedule(
    execution::sender auto predecessor,
    execution::scheduler auto scheduler
);

execution::sender auto lazy_reschedule(
    execution::sender auto predecessor,
    execution::scheduler auto scheduler
);
</pre>

Returns a sender describing the transition from the execution agent of the predecessor to the execution agent of the target scheduler. See [[#design-transitions]].

<pre highlight="c++">
execution::scheduler auto cpu_sched = get_system_thread_pool().scheduler();
execution::scheduler auto gpu_sched = cuda::scheduler();

execution::sender auto cpu_task = execution::schedule(cpu_sched);
// cpu_task describes the creation of a new task on the system thread pool

execution::sender auto gpu_task = execution::reschedule(cpu_task, gpu_sched);
// gpu_task describes the transition of the task graph described by cpu_task to the gpu
</pre>

### `execution::then` ### {#design-adapter-then}

<pre highlight="c++">
execution::sender auto then(
    execution::sender auto predecessor,
    std::invocable<<i>values-sent-by(predecessor)</i>...> function
);

execution::sender auto lazy_then(
    execution::sender auto predecessor,
    std::invocable<<i>values-sent-by(predecessor)</i>...> function
);
</pre>

Returns a sender describing the task graph described by the predecessor, with an added node of invoking the provided function with the values [=send|sent=] by the predecessor as arguments.

`lazy_then` is **guaranteed** to not begin executing `function` until the returned sender is started.

<pre highlight="c++">
execution::sender auto pred = get_predecessor();
execution::sender auto snd = execution::then(pred, [](auto... args) {
    std::print(args..);
});
// snd describes the work described by pred
// followed by printing all of the values sent by pred
</pre>

This algorithm is included, as it is necessary for writing any sender code that actually performs a useful function.

### `execution::on` ### {#design-adapter-on}

<pre highlight="c++">
execution::sender auto on(
    execution::scheduler auto sched,
    execution::sender auto snd
);

execution::sender auto lazy_on(
    execution::scheduler auto sched,
    execution::sender auto snd
);
</pre>

Returns a sender which, when started, will start the provided sender on an execution agent belonging to the execution context associated with the provided scheduler. This returned sender does not have a [=completion scheduler=].

TODO: example

### `execution::unschedule` ### {#design-adapter-unschedule}

<pre highlight="c++">
execution::sender unschedule(
    execution::sender auto snd
);
</pre>

Returns a sender which [=send|sends=] values equivalent to values sent by the provided sender, but does not have an [=completion scheduler=].

TODO: example

### `execution::bulk` ### {#design-adapter-bulk}

<pre highlight="c++">
execution::sender auto bulk(
    execution::sender auto predecessor,
    std::integral auto size,
    invocable&lt;decltype(size), <i>values-sent-by(predecessor)</i>...> function
);

execution::sender auto lazy_bulk(
    execution::sender auto predecessor,
    std::integral auto size,
    invocable&lt;decltype(size), <i>values-sent-by(predecessor)</i>...> function
);
</pre>

Returns a sender describing the task of invoking the provided function with the values [=send|sent=] by the predecessor for every index in the provided shape.

In this paper, only integral types satisfy the concept of a shape, but future papers will explore bulk shapes of different kinds in more detail.

`lazy_bulk` is **guaranteed** to not begin executing `function` until the returned sender is started.

XXX TODO: provide an example

### `execution::split` ### {#design-adapter-split}

<pre highlight="c++">
execution::sender auto split(execution::sender auto sender);

execution::sender auto lazy_split(execution::sender auto sender);
</pre>

If the provided sender is a multi-shot sender, returns that sender. Otherwise, returns a multi-shot sender which sends values equivalent to the values sent by the provided sender. See [[#design-fork]].

### `execution::when_all` ### {#design-adapter-when_all}

<pre highlight="c++">
execution::sender auto when_all(
    execution::sender auto ...predecessors
)</i>);
</pre>

Returns a sender which completes once all of the predecessor senders have completed. The values send by this sender are the values sent by each of the predecessor, in order of the arguments passed to `when_all`.

The returned sender does not have a [=completion scheduler=].

`when_all` is a strictly lazy algorithm; it is guaranteed to not start any of the predecessor senders until the returned sender is started.

See [[#design-join]].

<pre highlight="c++">
execution::scheduler auto sched = get_thread_pool().scheduler();

execution::sender auto sends_1 = ...;
execution::sender auto sends_abc = ...;

execution::sender auto both = execution::when_all(sched,
    sends_1,
    sends_abc
);

execution::sender auto final = execution::then(both, [](auto... args){
    std::cout << std::format("the two args: {}, {}", args...);
});
// when final executes, it will print "the two args: 1, abc"
</pre>

### `execution::reschedule_when_all` ### {#design-adapter-reschedule_when_all}

<pre highlight="c++">
execution::sender auto reschedule_when_all(
    execution::scheduler auto sched,
    execution::sender auto ...predecessors
);

execution::sender auto lazy_reschedule_when_all(
    execution::scheduler auto sched,
    execution::sender auto ...predecessors
);
</pre>

Similar to [[#design-adapter-when_all]], but returns a sender whose [=completion scheduler=] is the provided scheduler.

See [[#design-join]].

### `execution::ensure_started` ### {#design-adapter-ensure_started}

<pre highlight="c++">
execution::sender auto ensure_started(
    execution::sender auto sender
);
</pre>

Once `ensure_started` returns, it is known that the provided sender has been [=connect|connected=] and `start` has been called on the resulting operation state (see [[#design-states]]); in other words, the work described by the provided sender has been submitted
for execution on the appropriate execution contexts. Returns a sender which completes when the provided sender completes and sends values equivalent to those of the provided sender.

XXX TODO: provide an example

## Proposed set of user-facing sender algorithms ## {#design-algorithms}

A <dfn>sender algorithm</dfn> is a function that accepts one or more senders.

### `execution::submit` ### {#design-algorithm-submit}

<pre highlight="c++">
void auto submit(
    execution::sender auto sender
);
</pre>

Like `ensure_started`, but does not return a value; if the provided sender sends an error instead of a value, `std::terminate` is called.

### `std::this_thread::sync_wait` ### {#design-algorithm-sync_wait}

<pre highlight="c++">
auto sync_wait(
    execution::sender auto sender
) requires (<i>always-sends-same-values</i>(sender))
    -> std::optional&lt;std::tuple&lt;<i>values-sent-by</i>(sender)>>;
</pre>

`std::this_thread::sync_wait` is an algorithm that submits the work described by the provided sender for execution, similarly to `ensure_started`, execept that it blocks <b>the current `std::thread` or thread of `main`</b> until the work is completed, and returns
an optional tuple of values that were sent by the provided sender on its completion of work. Where [[#design-factory-schedule]] and [[#design-factory-reschedule_just]] are meant to <i>enter</i> the domain of senders, `sync_wait` is meant to <i>exit</i> the domain of
senders, retrieving the result of the task graph.

If the provided sender sends an error instead of values, `sync_wait` throws that error as an exception, or rethrows the original exception if the error is of type `std::exception_ptr`.

If the provided sender sends the "done" signal instead of values, `sync_wait` returns an empty optional.

For an explanation of the `requires` clause, see [[#design-typed-issue]].

XXX TODO: provide an example

Note: Notice that this function is specified inside `std::this_thread`, and not inside `execution`. This is because `sync_wait` has to block the <i>current</i> execution agent, but determining what the current execution agent is is not reliable. Since the standard
does not specify any functions on the current execution agent other than those in `std::this_thread`, this is the flavor of this function that is being proposed. If C++ ever obtains fibers, for instance, we expect that a variant of this function called
`std::this_fiber::sync_wait` would be provided. We also expect that runtimes with execution agents that use different synchronization mechanisms than `std::thread`'s will provide their own flavors of `sync_wait` as well (assuming their execution agents have the means
to block in a non-deadlock manner).

## `execution::execute` ## {#design-execute}

In addition to the three categories of functions presented above, we also propose to include a convenience function for fire-and-forget eager one-way submission of an invocable to a scheduler, to fullfil the role of one-way executors from P0443.

<pre highlight="c++">
void execution::execute(
    execution::schedule auto sched,
    std::invocable<void> auto fn
);
</pre>

Submits the provided function for execution on the provided scheduler, as-if by:

<pre highlight="c++">
auto snd = execution::schedule(sched);
auto work = execution::then(snd, fn);
execution::submit(work);
</pre>

TODO: example

# Proposed design - implementer side # {#design-implementer}

## Receivers serve as glue between senders ## {#design-receivers}

A <dfn>receiver</dfn> is a callback that supports more than one channel. In fact, it supports three of them:

* `set_value`, which is the moral equivalent of an `operator()` or a function call, which signals successful completion of its predecessors;
* `set_error`, which signals that an error has happened during scheduling of the current work, executing the current work, or at some earlier point in the sender chain; and
* `set_done`, which signals that the *predecessor* requests that any work that has not started yet should not start, because it is not needed (this is cancellation, but propagating from the root of the execution tree down to its leaf nodes, as opposed to propagating
    from the leaf nodes up to the root - so **not** `some_sender.cancel()`).

Exactly one of these channels must be successfully (i.e. without an exception being thrown) invoked on a receiver before it is destroyed; if a call to `set_value` failed with an exception, either `set_error` or `set_done` must be invoked on the same receiver. These
requirements are know as the <dfn>receiver contract</dfn>.

While the receiver interface may look novel, it is in fact very similar to the interface of `std::promise`, which provides the first two signals as `set_value` and `set_error`, and it's possible to emulate the third channel with lifetime management of the promise.

Receivers are not a part of the end-user-facing API of this proposal; they are necessary to allow unrelated senders communicate with each other, but the only users who will interact with receivers directly are authors of senders.

Receivers are what is passed as the second argument to [[#design-connect]].

## Operation states represents work ## {#design-states}

An <dfn>operation state</dfn> is an object that represents work. Unlike senders, it is not a chaining mechanism; instead, it is a concrete object that packages the work described by a full sender chain, ready to be executed. An operation state is neither movable nor
copyable, and its interface consists of a single operation: `start`, which serves as the submission point of the work represented by a given operation state.

Operation states are not a part of the user-facing API of this proposal; they are necessary for implementing algorithms like `ensure_started` and `std::this_thread::sync_wait`, and the knowledge of them is necessary to implement senders, so the only users who will
interact with operation states directly are authors of senders and authors of sender algorithms.

The return value of [[#design-connect]] must satisfy the operation state concept.

## `execution::connect` ## {#design-connect}

`execution::connect` is a customization point which <dfn lt="connect">connects</dfn> senders with receivers, resulting in an operation state that will ensure that the [=receiver contract=] of the receiver passed to `connect` will be fulfilled.

<pre highlight="c++">
execution::sender auto snd = <i>some predecessor sender</i>;
execution::receiver auto rcv = <i>some receiver</i>;
execution::operation_state auto state = execution::connect(snd, rcv);

execution::start(state);
// at this point, it is guaranteed that the work represented by state has been submitted
// to an execution context, and that execution context will eventually fulfill the
// receiver contract of rcv

// operation states are not movable, and therefore this operation state object must be
// kept alive until the operation finishes
</pre>

## Sender algorithms are customizable ## {#design-customization}

Senders being able to be bound to schedulers, and being able to advertise what their [=completion scheduler=] is, fulfills one of the promises of senders: that of being able to customize an implementation of an algorithm based on what scheduler it will execute on.

The simple way to provide customizations for functions like `then`, that is for [=sender adapter|sender adapters=] and [=sender algorithm|algorithms=], is to follow the customization scheme that has been adopted for C++20 Ranges library; to do that, we would define
the expression `execution::then(sender, invocable)` to be equivalent to:

  1. `sender.then(invocable)`, if that expression is well formed; otherwise
  2. `then(sender, invocable)`, performed in a context where this call always performs ADL, if that expression is well formed; otherwise
  3. a default implementation of `then`, which returns a sender adapter, and then define the exact semantics of said adapter.

However, this definition is problematic. Imagine another sender algorithm, `bulk`, which is a structured abstraction for a loop over an index space. Its default implementation is just a for loop. However, for accelerator runtimes like CUDA, we would like operations
like `bulk` to have specialized behavior, which invokes a kernel of more than one thread (with its size defined by the call to `bulk`); therefore, we would like to customize `bulk` for CUDA senders to achieve this. However, there's no reason for CUDA kernels to
necessarily customize the `then` algorithm, as the generic implementation is perfectly sufficient. This creates a problem, though; consider the following snippet:

<pre highlight="c++">
execution::scheduler auto cuda_sch = cuda_scheduler{};

execution::sender auto initial = execution::schedule(cuda_sch);
// the type of initial is a type defined by the cuda_scheduler
// let's call it cuda::schedule_sender&lt;>

execution::sender auto next = execution::then(cuda_sch, []{ return 1; });
// the type of next is a standard-library implementation-defined sender adapter
// that wraps the cuda sender
// let's call it execution::then_sender_adapter&lt;cuda::schedule_sender&lt;>>

execution::sender auto kernel_sender = execution::bulk(next, shape, [](int i){ ... });
</pre>

How can we specialize the `bulk` algorithm for our wrapped `schedule_sender`? Well, here's one possible approach, taking advantage of ADL (and the fact that the definition of "associated namespace" also recursively enumerates the associated namespaces of all template
parameters of a type):

<pre highlight="c++">
namespace cuda::for_adl_purposes {
template&lt;typename... SentValues>
class schedule_sender {
    execution::operation_state auto connect(execution::receiver auto rcv);
    execution::scheduler auto get_scheduler() const;
};

execution::sender auto bulk(
    execution::sender auto && predecessor,
    execution::shape auto && shape,
    invocable<<i>sender-values(predecessor)</i>> auto && fn)
{
    // return a cuda sender representing a bulk kernel launch
}
}
</pre>

However, if the predecessor is not just a `then_sender_adapter` like in the example above, but another sender that overrides `bulk` by itself, as a member function, because its author believes they know an optimization for bulk - the specialization above will no
longer be selected, because a member function of the first argument is a better match than the ADL-found overload.

This means that well-meant specialization of algorithms on senders that are entirely scheduler-agnostic can have negative consequences; the scheduler-specific specialization - which is essential for good performance on platforms providing specialized ways to launch
certain algorithms - would not be selected in such cases. But it's really the scheduler that should control the behavior of algorithms when a non-default implementation exists, not the sender. Senders merely describe work; schedulers, however, are the handle to the
runtime that will eventually execute said work, and should thus have the final say in *how* the work is going to be executed.

Therefore, we are proposing the following customization scheme: the expression `execution::<algorithm>(sender, args...)`, for any given algorithm (accepting a sender as its first argument), should be equivalent to:

  1. `get_scheduler(sender).<algorithm>(sender, args...)`, if that expression is well-formed; otherwise
  2. `sender.<algorithm>(args...)`, if that expression is well-formed; otherwise
  3. `<algorithm>(sender, args...)`, performed in a context where this call always performs ADL, if that expression is well formed; otherwise
  4. a default implementation, if there exists a default implementation of the given algorithm.

For algorithms which accept concepts other than `sender` as their first argument, we propose that the customization scheme remains as it has been in [[P0443R14]] so far.

TODO: more example

## Laziness of operations is defined by algorithms ## {#design-laziness}

We distinguish two different guarantees about *when* work is submitted to an execution context:

 * <dfn>strictly lazy submission</dfn>, which means that there is a guarantee that no work is submitted to an execution context before a receiver is connected to a sender, and `execution::start` is called on the resulting operation state;
 * <dfn>potentially eager submission</dfn>, which means that work may be submitted to an execution context as soon as all the information necessary to perform it is provided.

If an operation requires potentially eager submission, strictly lazy submission is acceptable as an implementation, because it does fulfill the potentially eager guarantee. This is why the default implementations for the non-strictly-lazy sender adapters are specified
to dispatch to the strictly lazy ones; for an author of a specific sender, it is sufficient to specialize the strictly lazy version, to also achieve a specialization of the potentially eager one.

As has been described in [[#design-adapters]], whether an operation is guaranteed to perform strictly lazy submission or not is defined by the adapter used to perform it; the adapters whose names begin with `lazy_` provide the strictly lazy guarantee.

## Lazy senders provide optimization opportunities ## {#design-fusion}

Because lazy senders fundamentally *describe* work, instead of describing or representing the submission of said work to an execution context, and thanks to the flexibility of the customization of most sender algorithms, they provide an opportunity for fusing
multiple operations in a sender chain together, into a single function that can later be submitted for execution by an execution context. There are two ways this can happen.

The first (and most common) way for such optimizations to happen is thanks to the structure of the implementation: because all the work is done within callbacks invoked on the completion of a predecessor's work completion, recursively up to the root of computation,
the compiler is able to see a chain of work described using senders as a tree of tail calls, allowing for inlining and removal of most of the sender machinery. In fact, when work is not submitted to execution contexts outside of the current thread of execution,
compilers are capable of removing the senders abstraction entirely, while still allowing for composition of functions across different parts of a program.

The second way for this to occur is when an algorithm is specialized for a specific set of arguments. For instance, we expect that, for senders which are known to have been started already, [[#design-adapter-ensure_started]] will be an identity transformation,
because the algorithm will be specialized for such senders. Similarly, an implementation could recognize two subsequent lazy [[#design-adapter-bulk]] operations of compatible shapes, and merge them together into a single submission of a GPU kernel.

## Execution context transitions are two-step ## {#design-transition-details}

Because `execution::reschedule` takes a sender as its first argument, it is not actually directly customizable by the target scheduler. This is by design: the target scheduler may not know how to transition <i>from</i> a scheduler such as a CUDA scheduler;
transitioning away from a GPU in an efficient manner requires making runtime calls that are specific to the GPU in question, and the same is usually true for other kinds of accelerators too (or for scheduler running on remote systems). To avoid this problem,
specialized schedulers like the ones mentioned here can still hook into the transition mechanism, and inject a sender which will perform a transition to the regular CPU execution context, so that any sender can be attached to it.

This, however, is a problem: because customization of algorithms must be controlled by the scheduler they will run on (see [[#design-customization]]), the type of the sender returned from `reschedule` must be controllable by the target scheduler. Besides, the target
scheduler may itself represent a specialized execution context, which requires additional work to be performed to transition <i>to</i> it. GPUs and remote node schedulers are once again good examples of such schedulers: executing code on their execution contexts
requires making runtime API calls for work submission, and quite possibly for the data movement of the values being sent by the predecessor passed into `reschedule`.

To allow for such customization from both ends, we propose the inclusion of a secondary transitioning algorithm, called `schedule_from`. This algorithm is a form of `schedule`, but takes an additional, second argument: the predecessor sender. This algorithm is not
meant to be invoked manually by the end users; they are always supposed to invoke `reschedule`, to ensure that both schedulers have a say in how the transitions are made. Any predecessor scheduler that specializes `reschedule(snd, sch)` shall ensure that the
return value of their customization is equivalent to `schedule_from(sch, snd2)`, where `snd2` is a successor of `snd` that sends values equivalent to those sent by `snd`.

The default implementation of `reschedule(snd, sched)` is `schedule_from(sched, snd)`.

# Proposed design - API specifics # {#design-api}

## Senders are pipeable ## {#design-pipelines}

All sender algorithms that take a single predecessor that we are proposing here take the predecessor as the first argument. This is not an accident; in line with prior proposals, we are proposing that senders and sender algorithms use the familiar pipeline syntax
for chaining, just like C++20 ranges do. Therefore, a user could write the following to represent a sequence of tasks to execute first on the CPU, then on a CUDA GPU, then on a CPU again, retrieving the result at the end:

<pre highlight=c++>
auto work = execution::schedule(get_thread_pool())
    | execution::then([]{ return 123; })
    | execution::reschedule(cuda::get_scheduler())
    | execution::then([](int i){ return 123 * 5; })
    | execution::reschedule(get_thread_pool())
    | execution::then([](int i){ return i - 5; });
auto [result] = std::this_thread::sync_wait(work);
// result == 610
</pre>

Certain functions should not be pipeable, because using the pipeline syntax can result in confusion of the semantics of the algorithms involved. Specifically, the following algorithms should not be pipeable:

 * `sync_wait`: rather than being a terminal node in a pipeline, it <b>consumes</b> a pipeline, and reads better as a function call;
 * `on` and `lazy_on`: the `on` algorithm changes how the sender passed to it is executed, not what happens to its result, but allowing it in a pipeline makes it read as if it performed a function more similar to `reschedule`.

## Senders are typed ## {#design-typed}

All senders must advertise the types they will [=send=] when they complete. This is necessary for a number of features, and writing code in a way that's agnostic of whether a predecessor is typed or not in common sender adapters such as `execution::then` is hard.
We are not aware of compelling use cases that *require* untyped senders (i.e. senders that effectively only decide what to send when you [=connect=] a receiver to them, and not without that), therefore we are proposing that all senders must be typed.

The mechanism for this advertisement is the same as in [[P0443R14]]; the way to query the types is through `sender_traits::value_types<tuple_like, variant_like>`.

### Design issue: `value_types` ### {#design-typed-issue}

`sender_traits::value_types` is a template that takes two arguments: one is a tuple-like template, the other is a variant-like template. The tuple-like argument is required to represent senders sending more than one value (such as `when_all`). The variant-like
argument is required to represent senders that choose which specific values to send at runtime.

There's a choice made in the specification of [[#design-algorithm-sync_wait]]: it returns a tuple of values sent by the sender passed to it, wrapped in `std::optional` to handle the `set_done` signal. However, this assumes that those values can be represented as a
tuple, like here:

<pre highlight=c++>
execution::sender auto sends_1 = ...;
execution::sender auto sends_2 = ...;
execution::sender auto sends_3 = ...;

auto [a, b, c] = std::this_thread::sync_wait(
    execution::when_all(get_scheduler(sends_1),
        sends_1,
        sends_2,
        sends_3
    ).value());
// a == 1
// b == 2
// c == 3
</pre>

This works well for senders that always send the same set of arguments. If we ignore the possibility of having a sender that sends different sets of arguments into a receiver, we can specify the "canonical" (i.e. required to be followed by all senders) form of
`value_types` of a sender which sends `Types...` to be as follows:

<pre highlight=c++>
template&lt;template&lt;typename ...> typename TupleLike>
using value_types = TupleLike<Types...>;
</pre>

If senders could only ever send one specific set of values, this would probably need to be the required form of `value_types` for all senders; defining it otherwise would cause very weird results and should be considered a bug.

This matter is somewhat complicated by the fact that (1) `set_value` for receivers can be overloaded and accept different sets of arguments, and (2) senders are allowed to send multiple different sets of values, depending on runtime conditions, the data they
consumed, and so on. To accomodate this, [[P0443R14]] also includes a second template parameter to `value_types`, one that represents a variant-like type. If we permit such senders, we would almost certainly need to require that the canonical form of `value_types`
for *all* senders (to ensure consistency in how they are handled, and to avoid accidentally interpreting a user-provided variant as a sender-provided one) sending the different sets of arguments `Types1...`, `Types2...`, ..., `TypesN...` to be as follows:

<pre highlight=c++>
template&lt;
    template&lt;typename ...> typename TupleLike,
    template&lt;typename ...> typename VariantLike
>
using value_types = VariantLike&lt;
    TupleLike&lt;Types1...>,
    TupleLike&lt;Types2...>,
    ...,
    TupleLike&lt;Types3...>
>;
</pre>

This, however, introduces a couple of complications:

1. A `just(1)` sender would also need to follow this structure, so the correct type for storing the value sent by it would be `std::variant<std::tuple<int>>` or some such. This introduces a lot of compile time overhead for the simplest senders, and this overhead
    effectively exists in all places in the code where `value_types` is queried, regardless of the tuple-like and variant-like templates passed to it. Such overhead does exist if only the tuple-like parameter exists, but is made much worse by adding this second
    wrapping layer.
2. As a consequence of (1): because `sync_wait` needs to store the above type, it can no longer return just a `std::tuple<int>` for `just(1)`; it has to return `std::variant<std::tuple<int>>`. C++ currently does not have an easy way to destructure this; it may get
    less awkward with pattern matching, but even then it seems extremely heavyweight to involve variants in this API, and for the purpose of generic code, the kind of the return type of `sync_wait` must be the same across all sender types.

One possible solution to (2) above is to place a requirement on `sync_wait` that it can only accept senders which send only a single set of values, therefore removing the need for `std::variant` to appear in its API; because of this, we propose to expose both
`sync_wait`, which is a simple, user-friendly version of the algorithm, but requires that `value_types` have only one possible variant, and `sync_wait_with_variant`, which accepts any sender, but returns an optional whose value type is the variant of all the possible
tuples sent by the predecessor sender:

<pre highlight=c++>
auto sync_wait_with_variant(
    execution::sender auto sender
) -> std::optional&lt;std::variant&lt;
        std::tuple&lt;<i>values<sub>0</sub>-sent-by</i>(sender)>,
        std::tuple&lt;<i>values<sub>1</sub>-sent-by</i>(sender)>,
        ...,
        std::tuple&lt;<i>values<sub>n</sub>-sent-by</i>(sender)>
    >>;

auto sync_wait(
    execution::sender auto sender
) requires (<i>always-sends-same-values</i>(sender))
    -> std::optional&lt;std::tuple&lt;<i>values-sent-by</i>(sender)>>;
</pre>

## Ranges-style CPOs vs `tag_invoke` ## {#design-dispatch}

XXX TODO: describe pros and cons of both; make the argument for using `tag_invoke`

short list:
1. allows for propagation of all specializations to a wrapped object, without having to know them all;
2. in the future provides what is necessary for a polymorphic wrapper that is supposed to perform well;
3. because we anticipate multiple versions of `sync_wait`, for different runtimes (`std::this_thread` vs `std::this_fiber`, for instance), the specializations for it must happen not by a global name (because otherwise it'd need to be something like
    `std_this_thread_sync_wait`), but rather by a namespaced name, and `tag_invoke` provides a way to do this

# Specification # {#spec}

Much of this wording follows the wording of [[P0443R14]]. [[#spec-execution]] is meant to be added as a new library clause to the working draft of C++.

# Execution control library <b>[execution]</b> # {#spec-execution}

1. This Clause describes components supporting execution of function objects [function.objects].

2. The following subclauses describe the requirements, concepts, and components for execution control primitives as summarized in Table 1.

<table>
<caption>Table 1: Execution control library summary <b>[tab:execution.summary]</b></caption>
<th><td>Subclause</td><td>Header</td></th>
<tr style="border-bottom-style: hidden;"><td><a href="#spec-execution.req">[execution.req]</a></td><td>Requirements</td><td>`<execution>`</td></tr>
<tr style="border-bottom-style: hidden;"><td><a href="#spec-execution.scheduler">[execution.scheduler]</a></td><td>Schedulers</td><td></td></tr>
<tr style="border-bottom-style: hidden;"><td><a href="#spec-execution.receiver">[execution.receiver]</a></td><td>Receivers</td><td></td></tr>
<tr style="border-bottom-style: hidden;"><td><a href="#spec-execution.op_state">[execution.op_state]</a></td><td>Operation states</td><td></td></tr>
<tr style="border-bottom-style: hidden;"><td><a href="#spec-execution.sender">[execution.sender]</a></td><td>Senders</td><td></td></tr>
<tr><td><a href="#spec-execution.execute">[execution.execute]</a></td><td>One-way execution</td><td></td></tr>
</table>

## Header `<execution>` synopsis <b>[execution.syn]</b> ## {#spec-execution.syn}

<pre highlight=c++>
namespace std::execution {
  // [execution.helpers], helper concepts
  template&lt;class T>
    concept <i>moveable-value</i> = <i>see-below</i>; // exposition only

  // [execution.scheduler], schedulers
  template&lt;class S>
    concept scheduler = <i>see-below</i>;

  inline namespace <i>unspecified</i> {
    struct get_forward_progress_guarantee_t;
    inline constexpr get_forward_progress_guarantee_t get_forward_progress_guarantee{}; // TODO
  }

  // [execution.receiver], receivers
  template&lt;class T, class E = exception_ptr>
    concept receiver = <i>see-below</i>;

  template&lt;class T, class... An>
    concept receiver_of = <i>see-below</i>;

  inline namespace <i>unspecified</i> {
    struct set_value_t;
    inline constexpr set_value_t set_value{};
    struct set_error_t;
    inline constexpr set_error_t set_error{};
    struct set_done_t;
    inline constexpr set_done_t set_done{};
  }

  // [execution.receiver.queries], receiver queries
  inline namespace <i>unspecified</i> {
    struct get_scheduler_t;
    inline constexpr get_scheduler_t get_scheduler{}; // TODO
    struct get_allocator_t;
    inline constexpr get_allocator_t get_allocator{}; // TODO
    struct get_stop_token_t;
    inline constesxpr get_stop_token_t get_stop_token{}; // TODO
  }

  // [execution.op_state], operation states
  template&lt;class O>
    concept operation_state = <i>see-below</i>;

  inline namespace <i>unspecified</i> {
    struct start_t;
    inline constexpr start_t start{};
  }

  // [execution.sender], senders
  template&lt;class S>
    concept sender = <i>see-below</i>;

  template&lt;class S, class R>
    concept sender_to = <i>see-below</i>;

  template&lt;class S>
    concept <i>has-sender-types</i> = <i>see-below</i>; // exposition only

  template&lt;class S>
    concept typed_sender = <i>see-below</i>;

  // [execution.sender.traits], sender traits
  template&lt;class S>
    struct sender_traits; // TODO

  inline namespace <i>unspecified</i> {
    // [execution.sender.connect], the connect algorithm
    struct connect_t;
    inline constexpr connect_t connect{};

    // [execution.sender.queries], sender queries
    template&lt;class CPO>
    struct get_completion_scheduler_t;
    template&lt;class CPO>
    inline constexpr get_completion_scheduler_t get_completion_scheduler{}; // TODO

    // [execution.sender.factories], sender factories
    struct schedule_t;
    inline constexpr schedule_t schedule{};
    template&lt;class... Ts>
      struct <i>just-sender</i>; // exposition only
    template&lt;<i>moveable-value</i>... Ts>
      <i>just-sender</i>&lt;remove_cvref_t&lt;Ts>...> just(Ts &&...);
    struct reschedule_just_t;
    inline constexpr reschedule_just_t reschedule_just{};

    // [execution.sender.adapters], sender adapters
    struct on_t;
    inline constexpr on_t on{};
    struct lazy_on_t;
    inline constexpr lazy_on_t lazy_on{};
    struct reschedule_t;
    inline constexpr reschedule_t reschedule{};
    struct lazy_reschedule_t;
    inline constexpr lazy_reschedule_t lazy_reschedule{};
    struct schedule_from_t;
    inline constexpr schedule_from_t schedule_from{};
    struct lazy_schedule_from_t;
    inline constexpr lazy_schedule_from_t lazy_schedule_from{};

    struct then_t;
    inline constexpr then_t then{};
    struct lazy_then_t;
    inline constexpr lazy_then_t lazy_then{};
    struct upon_error_t;
    inline constexpr upon_error_t upon_error{};
    struct lazy_upon_error_t;
    inline constexpr lazy_upon_error_t lazy_upon_error{};
    struct upon_done_t;
    inline constexpr upon_done_t upon_done{};
    struct lazy_upon_done_t;
    inline constexpr lazy_upon_done_t lazy_upon_done{};

    struct bulk_t;
    inline constexpr bulk_t bulk{};
    struct lazy_bulk_t;
    inline constexpr lazy_bulk_t lazy_bulk{};

    struct split_t;
    inline constexpr split_t split{}; // TODO
    struct lazy_split_t;
    inline constexpr lazy_split_t lazy_split{}; // TODO
    struct when_all_t;
    inline constexpr when_all_t when_all{};
    struct lazy_when_all_t;
    inline constexpr lazy_when_all_t lazy_when_all{};
    struct reschedule_when_all_t;
    inline constexpr reschedule_when_all_t reschedule_when_all{};
    struct lazy_reschedule_when_all_t;
    inline constexpr lazy_reschedule_when_all_t lazy_reschedule_when_all{};

    struct unschedule_t;
    inline constexpr unschedule_t unschedule{}; // TODO
    struct ensure_started_t;
    inline constexpr ensure_started_t ensure_started{}; // TODO

    // [execution.sender.algorithms], sender algorithms
    struct start_detached_t;
    inline constexpr start_detached_t start_detached{}; // TODO
  }
}

namespace std::this_thread {
  inline namespace <i>unspecified</i> {
    struct sync_wait_t;
    inline constexpr sync_wait_t sync_wait{}; // TODO
    struct sync_wait_with_variant_t;
    inline constexpr sync_wait_with_variant_t sync_wait_with_variant{}; // TODO
  }
}

namespace std::execution {
  inline namespace <i>unspecified</i> {
    // [execution.execute], one-way execution
    struct execute_t;
    inline constexpr execute_t execute{}; // TODO
  }
}
</pre>

## Helper concepts <b>[execution.helpers]</b> ## {#spec-execution.helpers}

    <pre highlight=c++>
    template&lt;class T>
    concept moveable-value = // exposition only
      move_constructible&lt;remove_cvref_t&lt;T>> &&
      constructible_from&lt;remove_cvref_t&lt;T>, T>;
    </pre>

## Schedulers <b>[execution.scheduler] </b> ## {#spec-execution.scheduler}

1. The `scheduler` concept defines the requirements of a type that allows for scheduling of work on its <i>associated execution context</i>.

    <pre highlight=c++>
    template&lt;class S>
      concept scheduler =
        copy_constructible&lt;remove_cvref_t&lt;S>> &&
        equality_comparable&lt;remove_cvref_t&lt;S>> &&
        requires(S&& s) {
          execution::schedule((S&&)s);
        };
    </pre>

2. None of a scheduler's copy constructor, destructor, equality comparison, or `swap` operations shall exit via an exception.

3. None of these operations, nor a scheduler type's `schedule` function, or a call to a scheduler type's query functions ([[#spec-execution.scheduler.queries]]) shall introduce data races as a result of concurrent invocations of thos functions from different
    threads.

4. For any two (possibly const) values `s1` and `s2` of some scheduler type `S`, `s1 == s2` shall return `true` only if both `s1` and `s2` are handles to the same associated execution context.

5. A scheduler type's destructor shall not block pending completion of any receivers connected to the sender objects returned from `schedule`. [<i>Note:</i> The ability to wait for completion of submitted function objects may be provided by the associated execution
    context of the scheduler. <i>—end note</i>]

## Receivers <b>[execution.receiver]</b> ## {#spec-execution.receiver}

1. A <i>receiver</i> represents the continuation of an asynchronous operation. An asynchronous operation may complete with a (possibly empty) set of values, an error, or it may be cancelled. A receiver has three principal operations corresponding to the three ways
    an asynchronous operation may complete: `set_value`, `set_error`, and `set_done`. These are collectively known as a receiver’s <i>completion-signal operations</i>.

2. The `receiver` concept defines the requirements for a receiver type with an unknown set of value types. The `receiver_of` concept defines the requirements for a receiver type with a known set of value types, whose error type is `std::exception_ptr`.

    <pre highlight=c++>
    template&lt;class T, class E = exception_ptr>
    concept receiver =
      move_constructible&lt;remove_cvref_t&lt;T>> &&
      constructible_from&lt;remove_cvref_t&lt;T>, T> &&
      requires(remove_cvref_t&lt;T>&& t, E&& e) {
        { execution::set_done(std::move(t)) } noexcept;
        { execution::set_error(std::move(t), (E&&) e) } noexcept;
      };

    template&lt;class T, class... An>
    concept receiver_of =
      receiver&lt;T> &&
      requires(remove_cvref_t&lt;T>&& t, An&&... an) {
        execution::set_value(std::move(t), (An&&) an...);
      };
    </pre>

3. The receiver’s completion-signal operations have semantic requirements that are collectively known as the <i>receiver contract</i>, described below:

    1. None of a receiver’s completion-signal operations shall be invoked before `execution::start` has been called on the operation state object that was returned by `execution::connect` to connect that receiver to a sender.

    2. Once `execution::start` has been called on the operation state object, exactly one of the receiver’s completion-signal operations shall complete non-exceptionally before the receiver is destroyed.

    3. If `execution::set_value` exits with an exception, it is still valid to call `execution::set_error` or `execution::set_done` on the receiver, but it is no longer valid to call `execution::set_value` on the receiver.

4. Once one of a receiver’s completion-signal operations has completed non-exceptionally, the receiver contract has been satisfied.

### Set value algorithm <b>[execution.receiver.set_value]</b> ### {#spec-execution.receiver.set_value}

1. `execution::set_value` is used to send a <i>value completion signal</i> to a receiver.

2. The name `execution::set_value` denotes a customization point object. The expression `execution::set_value(R, Vs...)` for some subexpressions `R` and `Vs...` is expression-equivalent to:

    1. `tag_invoke(set_value, R, Vs...)`, if that expression is valid. If the function selected by `tag_invoke` does not send the value(s) `Vs...` to the receiver `R`’s value channel, the program is ill-formed with no diagnostic required.

    2. Otherwise, `execution::set_value(R, Vs...)` is ill-formed.

### Set error algorithm <b>[execution.receiver.set_error]</b> ### {#spec-execution.receiver.set_error}

1. `execution::set_error` is used to send a <i>error signal</i> to a receiver.

2. The name `execution::set_error` denotes a customization point object. The expression `execution::set_error(R, E)` for some subexpressions `R` and `E` is expression-equivalent to:

    1. `tab_invoke(set_error, R, E)`, if that expression is valid. If the function selected by `tag_invoke` does not send the error `E` to the receiver `R`’s error channel, the program is ill-formed with no diagnostic required.

    2. Otherwise, `execution::set_error(R, E)` is ill-formed.

### Set done algorithm <b>[execution.receiver.set_done]</b> ### {#spec-execution.receiver.set_done}

1. `execution::set_done` is used to send a <i>done signal</i> to a receiver.

2. The name `execution::set_done` denotes a customization point object. The expression `execution::set_done(R)` for some subexpression `R` is expression-equivalent to:

    1. `tag_invoke(set_done, R)`, if that expression is valid. If the function selected by `tag_invoke` does not signal the receiver `R`’s done channel, the program is ill-formed with no diagnostic required.

    2. Otherwise, `execution::set_done(R)` is ill-formed.

### Receiver queries <b>[execution.receiver.queries]</b> ### {#spec-execution.receiver.queries}

## Operation states <b>[execution.op_state]</b> ## {#spec-execution.op_state}

1. The `operation_state` concept defines the requirements for an operation state type, which allows for starting the execution of work.

    <pre highlight=c++>
    template&lt;class O>
      concept operation_state =
        destructible&lt;O> &&
        is_object_v&lt;O> &&
        requires (O& o) {
          { execution::start(o) } noexcept;
        };
    </pre>

### Start algorithm <b>[execution.op_state.start]</b> ### {#spec-execution.op_state.start}

1. `execution::start` is used to start work represented by an operation state object.

2. The name `execution::start` denotes a customization point object. The expression `execution::start(O)` for some lvalue subexpression `O` is expression-equivalent to:

    1. `tag_invoke(start, O)`, if that expression is valid. If the function selected by `tag_invoke` does not start the work represented by the operation state `O`, the program is ill-formed with no diagnostic required.

    2. Otherwise, `execution::start(O)` is ill-formed.

3. The caller of `execution::start(O)` must guarantee that the lifetime of the operation state object `O` extends at least until one of the receiver completion-signal functions of a receiver `R` passed into the `execution::connect` call that produced `O` is ready
    to successfully return. [<i>Note:</i> this allows for the receiver to manage the lifetime of the operation state object, if destroying it is the last operation it performs in its completion-signal functions. --<i>end note</i>]

## Senders <b>[execution.sender]</b> ## {#spec-execution.sender}

1. A sender describes a potentially asynchronous operation. A sender's responsibility is to fulfill the receiver contract of a connected receiver by delivering one of the receiver completion-signals.

2. The `sender` concept defines the requirements for a sender type. The `sender_to` concept defines the requirements for a sender type capable of being connected with a specific receiver type.

    <pre highlight=c++>
    template&lt;class S>
      concept sender =
        move_constructible&lt;remove_cvref_t&lt;S>> &&
        !requires {
          typename sender_traits&lt;remove_cvref_t&lt;S>>::__unspecialized; // exposition only
        };

    template&lt;class S, class R>
      concept sender_to =
        sender&lt;S> &&
        receiver&lt;R> &&
        requires (S&& s, R&& r) {
          execution::connect((S&&) s, (R&&) r);
        };
    </pre>

3. A sender is <i>typed</i> if it declares what types it sends through a connected receiver's channels.

4. The `typed_sender` concept defines the requirements for a typed sender type.

    <pre highlight=c++>
    template&lt;class S>
      concept <i>has-sender-types</i> = // exposition only
        requires {
          typename has-value-types&lt;S::template value_types>;
          typename has-error-types&lt;S::template error_types>;
          typename bool_constant&lt;S::sends_done>;
        };

    template&lt;class S>
      concept typed_sender =
        sender&lt;S> &&
        <i>has-sender-types</i>&lt;sender_traits&lt;remove_cvref_t&lt;S>>>;
    </pre>

### Sender traits <b>[execution.sender.traits]</b> ### {#spec-execution.sender.traits}

### Connect algorithm <b>[execution.sender.connect]</b> ### {#spec-execution.sender.connect}

1. `execution::connect` is used to <i>connect</i> a sender with a receiver, producing an operation state object that represents the work that needs to be performed to satisfy the receiver contract of the receiver with values that are the result of the operations
    described by the sender.

2. The name `execution::connect` denotes a customization point object. For some subexpressions `s` and `r`, let `S` be `decltype((s))` and `R` be `decltype((r))`. If `R` does not satisfy `execution::receiver` or `S` does not satisfy `execution::sender`,
    `execution::connect(s, r)` is ill-formed. Otherwise, the expression `execution::connect(s, r)` is expression-equivalent to:

    1. `tag_invoke(connect, s, r)`, if that expression is valid and its type satisfies `execution::operation_state`. If the function selected by `tag_invoke` does not return an operation state for which `execution::start` starts work described by `s`, the program
        is ill-formed with no diagnostic required.

    2. Otherwise, `execution::connect(s, r)` is ill-formed.

### Sender queries <b>[execution.sender.queries]</b> ### {#spec-execution.sender.queries}

### Sender factories <b>[execution.sender.factories]</b> ### {#spec-execution.sender.factories}

#### General <b>[execution.sender.factories.general]</b> #### {#spec-execution.sender.factories.general}

1. Subclause [execution.sender.factories] defines <i>sender factories</i>, which are utilities that return senders without accepting senders as arguments.

#### Schedule sender factory <b>[execution.sender.schedule]</b> #### {#spec-execution.sender.schedule}

1. `execution::schedule` is used to obtain a sender associated with a scheduler, which can be used to describe work to be started on that scheduler's associated execution context.

2. The name `execution::schedule` denotes a customization point object. For some subexpression `s`, let `S` be `decltype((s))`. If `S` does not satisfy `execution::scheduler`, `execution::schedule` is ill-formed. Otherwise, the expression `execution::schedule(s)`
    is expression-equivalent to:

    1. `tag_invoke(schedule, s)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected by `tag_invoke` does not return a sender whose completion scheduler is equivalent to `s`, the program is ill-formed with no diagnostic
        required.

    2. Otherwise, `execution::schedule(s)` is ill-formed.

#### Just sender factory <b>[execution.sender.just]</b> #### {#spec-execution.sender.just}

1. `execution::just` is used to create a sender that propagates a set of values to a connected receiver.

    <pre highlight=c++>
    template&lt;class... Ts>
    struct <i>just-sender</i> // exposition only
    {
      std::tuple&lt;Ts...> vs_;

      template&lt;template&lt;class...> class Tuple, template&lt;class...> class Tuple>
      using value_types = Variant&lt;Tuple&lt;Ts...>>;

      template&lt;template&lt;class...> class Variant>
      using error_types = Variant&lt;>;

      static const constexpr auto sends_done = false;

      template&lt;class R>
      struct operation_state {
        std::tuple&lt;Ts...> vs_;
        R r_;

        void tag_invoke(execution::start_t)
          noexcept(noexcept(
            execution::set_value(declval&lt;R>(), declval&lt;Ts>()...)
          )) {
          try {
            apply([&](Ts &... values_) {
              execution::set_value(move(r_), move(values_)...);
            }, vs_);
          }
          catch (...) {
            execution::set_error(move(r_), current_exception());
          }
        }
      };

      template&lt;receiver R>
        requires receiver_of&lt;R, Ts...> && (copyable&ltTs>... &&)
      auto tag_invoke(execution::connect_t, R && r) const & {
        return operation_state&lt;R>{ vs_, std::forward&lt;R>(r) };
      }

      template&lt;receiver R>
        requires receiver_of&lt;R, Ts...>
      auto tag_invoke(execution::connect_t, R && r) && {
        return operation_state&lt;R>{ std::move(vs_), std::forward&lt;R>(r) };
      }
    };

    template&lt;<i>moveable-value</i>... Ts>
      <i>just-sender</i>&lt;remove_cvref_t&lt;Ts>...> just(Ts &&... ts) noexcept(<i>see-below</i>);
    </pre>

1. <i>Effects</i>: Initializes `vs_` with `make_tuple(forward<Ts>(ts)...)`.

2. <i>Remarks</i>: The expression in the `noexcept-specifier` is equivalent to

    <pre highlight=c++>
    (is_nothrow_constructible_v&lt;remove_cvref_t&lt;Ts>, Ts> && ...)
    </pre>

#### Reschedule_just sender factory <b>[execution.sender.reschedule_just]</b> #### {#spec-execution.sender.reschedule_just}

1. `execution::reschedule_just` is used to create a sender that propagates a set of values to a connected receiver on an execution agent belonging to the associated execution context of a specified scheduler.

2. The name `execution::reschedule_just` denotes a customization point object. For some subexpressions `s` and `vs...`, let `S` be `decltype((s))` and `Vs...` be `decltype((vs))`. If `S` does not satisfy `execution::scheduler`, or any type `V` in `Vs` does not
    satisfy `<i>moveable-value</i>`, `execution::reschedule_just(s, vs...)` is ill-formed. Otherwise, `execution::reschedule_just(s, vs...)` is expression-equivalent to:

    1. `tag_invoke(reschedule_just, s, vs...)`, if that expression is valid and its type satisfies `execution::typed_sender`. If the function selected by `tag_invoke` does not return a sender whose completion scheduler is equivalent to `s` and sends values
        equivalent to `vs...` to a receiver connected to it, the program is ill-formed with no diagnostic required.

    2. Otherwise, `execution::reschedule(execution::just(vs...), s)`.

### Sender adaptors <b>[execution.sender.adaptors]</b> ### {#spec-execution.sender.adaptors}

#### General <b>[execution.sender.adaptors.general]</b> #### {#spec-execution.sender.adaptors.general}

1. Subclause [execution.sender.adaptors] defines <i>sender adaptors</i>, which are utilities that transform one or more senders into a sender with custom behaviors. When they accept a single sender argument, they can be chained to create sender chains.

2. The bitwise OR operator is overloaded for the purpose of creating sender chains. The adaptors also support function call syntax with equivalent semantics.

3. Most sender adaptors have two versions, an <i>potentially eager version</i>, and a <i>strictly lazy</i> version. For such sender adaptors, <code><i>adaptor</i></code> is the potentially eager version, and <code>lazy&lowbar;<i>adaptor</i></code> is the strictly
    lazy version.

4. A strictly lazy version of a sender adaptor is required to not begin executing any functions which would observe or modify any of the arguments of the adaptor before the returned sender is connected with a receiver using `execution::connect`, and
    `execution::start` is called on the resulting operation state. This requirement applies to any function that is selected by the implementation of the sender adaptor.

5. Unless otherwise specified, all sender adaptors which accept a single `sender` argument return sender objects that propagate sender queries to that single sender argument. This requirement applies to any function that is selected by the implementation of the
    sender adaptor.

6. Unless otherwise specified, whenever a strictly lazy sender adaptor constructs a receiver it passes to another sender's connect, that receiver shall propagate receiver queries to a receiver accepted as an argument of `execution::connect`. This requirements
    applies to any sender returned from a function that is selected by the implementation of a strictly lazy sender adaptor.

#### Sender adaptor closure objects <b>[execution.sender.adaptor.objects]</b> #### {#spec-execution.sender.adaptor.objects}

1. A <i>pipeable sender adaptor closure object</i> is a function object that accepts one or more `sender` arguments and returns a `sender`. For a sender adaptor closure object `C` and an expression `S` such that `decltype((S))` models `sender`, the following
    expressions are equivalent and yield a `sender`:

    <pre highlight=c++>
    C(S)
    S | C
    </pre>

    Given an additional pipeable sender adaptor closure object `D`, the expression `C | D` is well-formed and produces another range adaptor closure object such that the following two expressions are equivalent:

    <pre highlight=c++>
    S | C | D
    S | (C | D)
    </pre>

2. A <i>pipeable sender adaptor object</i> is a customization point object that accepts a `sender` as its first argument and returns a `sender`.

3. If a pipeable sender adaptor object accepts only one argument, then it is a pipeable sender adaptor closure object.

4. If a pipeable sender adaptor object accepts more than one argument, then the following expressions are equivalent:

    <pre highlight=c++>
    <i>adaptor</i>(sender, args...)
    <i>adaptor</i>(args...)(sender)
    sender | <i>adaptor</i>(args...)
    </pre>

    In that case, <code><i>adaptor</i>(args...)</code> is a pipeable sender adaptor closure object.

#### On adaptor <b>[execution.sender.adaptors.on]</b> #### {#spec-execution.sender.adaptors.on}

1. `execution::on` and `execution::lazy_on` are used to adapt a sender in a sender that will start the predecessor sender on an execution agent belonging to a specific execution context.

2. The name `execution::on` denotes a customization point object. For some subexpressions `sch` and `s`, let `Sch` be `decltype((sch))` and `S` be `decltype((s))`. If `Sch` does not satisfy `execution::scheduler`, or `S` does not satisfy `execution::sender`,
    `execution::on` is ill-formed. Otherwise, the expression `execution::on(sch, s)` is expression-equivalent to:

    1. `tag_invoke(on, sch, s)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `lazy_on(sch, s)`.

    If the function selected above does not return a sender which starts `s` on an execution agent of the associated execution context of `sch`, the program is ill-formed with no diagnostic required.

2. The name `execution::lazy_on` denotes a customization point object. For some subexpressions `sch` and `s`, let `Sch` be `decltype((sch))` and `S` be `decltype((s))`. If `Sch` does not satisfy `execution::scheduler`, or `S` does not satisfy `execution::sender`,
    `execution::lazy_on` is ill-formed. Otherwise, the expression `execution::lazy_on(sch, s)` is expression-equivalent to:

    1. `tag_invoke(lazy_on, sch, s)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected above does not return a sender which starts `s` on an execution agent of the associated execution context of `sch` when started,
        the program is ill-formed with no diagnostic required.

    2. Otherwise, constructs a sender `s2`. When `s2` is connected with some receiver `out_r`, it results in an operation state `op_state`. When `execution::start` is called on `op_state`, it:

        1. Constructs a receiver `r`:

            2. When `execution::set_value(r)` is called, it calls `execution::connect(s, out_r)`, which results in `op_state2`. It calls `execution::start(op_state2)`. If any of these operations throws an exception, it calls `execution::set_error` on `out_r`,
                passing `current_exception()` as the second argument.

            3. When `execution::set_error(r, e)` is called, it calls `execution::set_error(out_r, e)`.

            4. When `execution::set_done(r)` is called, it calls `execution::set_done(out_r)`.

        2. Calls `execution::schedule(sch)`, which results in `s3`. It then calls `execution::connect(s3, r)`, resulting in `op_state3`, and then it calls `execution::start(op_state3)`. If any of these operation throws an exception, it catches it and calls
            `execution::set_error(out_r, current_exception())`.

#### Reschedule adaptor <b>[execution.sender.adaptors.reschedule]</b> #### {#spec-execution.sender.adaptors.reschedule}

1. `execution::reschedule` and `execution::lazy_reschedule` are used to adapt a sender into a sender with a different associated completion scheduler. [<i>Note</i>: it results in a transition between different execution contexts when executed. --<i>end note</i>]

2. The name `execution::reschedule` denotes a customization point object. For some subexpressions `sch` and `s`, let `Sch` be `decltype((sch))` and `S` be `decltype((s))`. If `Sch` does not satisfy `execution::scheduler`. or `S` does not satisfy `execution::sender`,
    `execution::reschedule` is ill-formed. Otherwise, the expression `execution::reschedule(s, sch)` is expression-equivalent to:

    1. `tag_invoke(reschedule, get_completion_scheduler<set_value>(s), s, sch)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(reschedule, s, sch)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, `schedule_from(sch, s)`.

    If the function selected above does not return a sender which is a result of a call to `execution::schedule_from(sch, s2)`, where `s2` is a sender which sends equivalent to those sent by `s`, the program is ill-formed with no diagnostic required.

3. The name `execution::lazy_reschedule` denotes a customization point object. For some subexpressions `sch` and `s`, let `Sch` be `decltype((sch))` and `S` be `decltype((s))`. If `Sch` does not satisfy `execution::scheduler`. or `S` does not satisfy
    `execution::sender`, `execution::lazy_reschedule` is ill-formed. Otherwise, the expression `execution::lazy_reschedule(s, sch)` is expression-equivalent to:

    1. `tag_invoke(lazy_reschedule, get_completion_scheduler<set_value>(s), s, sch)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(lazy_reschedule, s, sch)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, `lazy_schedule_from(sch, s)`.

    If the function selected above does not return a sender which is a result of a call to `execution::lazy_schedule_from(sch, s2)`, where `s2` is a sender which sends equivalent to those sent by `s`, the program is ill-formed with no diagnostic required.

4. Senders returned from `execution::reschedule` and `execution::lazy_reschedule` shall not propagate the sender queries `get_completion_scheduler<CPO>` to a predecessor sender. They shall return a sender equivalent to the `sch` argument from those queries.

#### Schedule_from adaptor <b>[execution.sender.adaptors.schedule_from]</b> #### {#spec-execution.sender.adaptors.schedule_from}

1. `execution::schedule_from` and `execution::lazy_schedule_from` are used to schedule work dependent on the completion of a sender onto a scheduler's associated execution context. [<i>Note</i>: `schedule_from` and `lazy_schedule_from` are not meant to be used in
    user code; they are used in the implementation of `reschedule` and `lazy_reschedule`. -<i>end note</i>]

2. The name `execution::schedule_from` denotes a customization point object. For some subexpressions `sch` and `s`, let `Sch` be `decltype((sch))` and `S` be `decltype((s))`. If `Sch` does not satisfy `execution::scheduler`, or `S` does not satisfy
    `execution::typed_sender`, `execution::schedule_from` is ill-formed. Otherwise, the expression `execution::schedule_from(sch, s)` is expression-equivalent to:

    1. `tag_invoke(schedule_from, sch, s)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected by `tag_invoke` does not return a sender which completes on an execution agent belonging to the associated execution context
        of `sch` and sends signals equivalent to those sent by `s`, the program is ill-formed with no diagnostic required.

    2. Otherwise, `lazy_schedule_from(sch, s)`.

3. The name `execution::lazy_schedule_from` denotes a customization point object. For some subexpressions `sch` and `s`, let `Sch` be `decltype((sch))` and `S` be `decltype((s))`. If `Sch` does not satisfy `execution::scheduler`, or `S` does not satisfy
    `execution::typed_sender`, `execution::lazy_schedule_from` is ill-formed. Otherwise, the expression `execution::lazy_schedule_from(sch, s)` is expression-equivalent to:

    1. `tag_invoke(lazy_schedule_from, sch, s)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected by `tag_invoke` does not return a sender which completes on an execution agent belonging to the associated execution
        context of `sch` and sends signals equivalent to those sent by `s`, the program is ill-formed with no diagnostic required.

    2. Otherwise, constructs a sender `s2`. When `s2` is connected with some reciever `out_r`, it:

        1. Constructs a receiver `r`.

        2. Calls `execution::connect(s, r)`, which results in an operation state `op_state2`. If any of these operations throws an exception, calls `execution::set_error` on `out_r`, passing `current_exception()` as the second argument.

        3. When a receiver completion-signal <code><i>Signal</i>(r, args...)</code> is called, it constructs a receiver `r2`:

            1. When `execution::set_value(r2)` is called, it calls <code><i>Signal</i>(out_r, args...)</code>.

            2. When `execution::set_error(r2, e)` is called, it calls `execution::set_error(out_r, e)`.

            3. When `execution::done(r2)` is called, it calls `execution::set_done(out_r)`.

            It then calls `execution::schedule(sch)`, resulting in a sender `s3`. It then calls `execution::connect(s3, r2)`, resulting in an operation state `op_state3`. It then calls `execution::start(op_state3)`. If any of these operations throws an exception,
            it catches it and calls `execution::set_error(out_r, current_exception())`.

        3. Returns an operation state `op_state` that contains `op_state2`. When `execution::start(op_state)` is called, calls `execution::start(op_state2)`.

4. Senders returned from `execution::reschedule` and `execution::lazy_reschedule` shall not propagate the sender queries `get_completion_scheduler<CPO>` to a predecessor sender. They shall return a scheduler equivalent to the `sch` argument from those queries.

#### Then adaptor <b>[execution.sender.adaptors.then]</b> #### {#spec-execution.sender.adaptor.then}

1. `execution::then` and `execution::lazy_then` are used to attach invocables as continuation for successful completion of the predecessor sender.

2. The name `execution::then` denotes a customization point object. For some subexpressions `s` and `f`, let `S` be `decltype((s))`. If `S` does not satisfy `execution::sender`, `execution::then` is ill-formed. Otherwise, the expression `execution::then(s, f)` is
    expression-equivalent to:

    1. `tag_invoke(then, get_completion_scheduler<set_value>(s), s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(then, s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, `lazy_then(s, f)`.

    If the function selected above does not return a sender which invokes `f` with the result of the `set_value` signal of `s`, passing the return value as the value to any connected receivers, and propagates the other completion-signals sent by `s`, the program is
        ill-formed with no diagnostic required.

2. The name `execution::lazy_then` denotes a customization point object. For some subexpressions `s` and `f`, let `S` be `decltype((s))`. If `S` does not satisfy `execution::sender`, `execution::lazy_then` is ill-formed. Otherwise, the expression
    `execution::lazy_then(s, f)` is expression-equivalent to:

    1. `tag_invoke(lazy_then, get_completion_scheduler<set_value>(s), s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(lazy_then, s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, constructs a sender `s2`. When `s2` is connected with some receiver `out_r`, it:

        1. Constructs a receiver `r`:

            1. When `execution::set_value(r, args...)` is called, calls `invoke(f, args...)` and passes the result `v` to `execution::set_value(out_r, v)`. If any of these operations throws an exception, it catches it and calls
                `execution::set_error(out_r, current_exception())`.

            2. When `execution::set_error(r, e)` is called, calls `execution::set_error(out_r, e)`.

            3. When `execution::set_done(r)` is called, calls `execution::set_done(out_r)`.

        2. Calls `execution::connect(s, r)`, which results in an operation state `op_state2`. If this operation throws an exception, it catches it and calls `execution::set_error(out_r, current_exception())`.

        3. Returns an operation state `op_state` that contains `op_state2`. When `execution::start(op_state)` is called, calls `execution::start(op_state2)`.

    If the function selected above does not return a sender which invokes `f` with the result of the `set_value` signal of `s`, passing the return value as the value to any connected receivers, and propagates the other completion-signals sent by `s`, the program is
        ill-formed with no diagnostic required.

#### Upon_error adaptor <b>[execution.sender.adaptors.upon_error]</b> #### {#spec-execution.sender.adaptor.upon_error}

1. `execution::upon_error` and `execution::lazy_upon_error` are used to attach invocables as continuation for successful completion of the predecessor sender.

2. The name `execution::upon_error` denotes a customization point object. For some subexpressions `s` and `f`, let `S` be `decltype((s))`. If `S` does not satisfy `execution::sender`, `execution::upon_error` is ill-formed. Otherwise, the expression
    `execution::upon_error(s, f)` is expression-equivalent to:

    1. `tag_invoke(upon_error, get_completion_scheduler<set_value>(s), s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(upon_error, s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, `lazy_upon_error(s, f)`.

    If the function selected above does not return a sender which invokes `f` with the result of the `set_error` signal of `s`, passing the return value as the value to any connected receivers, and propagates the other completion-signals sent by `s`, the program is
        ill-formed with no diagnostic required.

2. The name `execution::lazy_upon_error` denotes a customization point object. For some subexpressions `s` and `f`, let `S` be `decltype((s))`. If `S` does not satisfy `execution::sender`, `execution::lazy_upon_error` is ill-formed. Otherwise, the expression
    `execution::lazy_upon_error(s, f)` is expression-equivalent to:

    1. `tag_invoke(lazy_upon_error, get_completion_scheduler<set_value>(s), s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(lazy_upon_error, s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, constructs a sender `s2`. When `s2` is connected with some receiver `out_r`, it:

        1. Constructs a receiver `r`:

            1. When `execution::set_value(r, args...)` is called, calls `execution::set_value(out_r, args...)`.

            2. When `execution::set_error(r, e)` is called, calls `invoke(f, e)` and passes the result `v` to `execution::set_value(out_r, v)`. If any of these operations throws an exception, it catches it and calls
                `execution::set_error(out_r, current_exception())`.

            3. When `execution::set_done(r)` is called, calls `execution::set_done(out_r)`.

        2. Calls `execution::connect(s, r)`, which results in an operation state `op_state2`. If this operation throws an exception, it catches it and calls `execution::set_error(out_r, current_exception())`.

        3. Returns an operation state `op_state` that contains `op_state2`. When `execution::start(op_state)` is called, calls `execution::start(op_state2)`.

    If the function selected above does not return a sender which invokes `f` with the result of the `set_error` signal of `s`, passing the return value as the value to any connected receivers, and propagates the other completion-signals sent by `s`, the program is
        ill-formed with no diagnostic required.

#### Upon_done adaptor <b>[execution.sender.adaptors.upon_done]</b> #### {#spec-execution.sender.adaptor.upon_done}

1. `execution::upon_done` and `execution::lazy_upon_done` are used to attach invocables as continuation for successful completion of the predecessor sender.

2. The name `execution::upon_done` denotes a customization point object. For some subexpressions `s` and `f`, let `S` be `decltype((s))`. If `S` does not satisfy `execution::sender`, `execution::upon_done` is ill-formed. Otherwise, the expression
    `execution::upon_done(s, f)` is expression-equivalent to:

    1. `tag_invoke(upon_done, get_completion_scheduler<set_value>(s), s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(upon_done, s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, `lazy_upon_done(s, f)`.

    If the function selected above does not return a sender which invokes `f` when the `set_done` signal of `s` is called, passing the return value as the value to any connected receivers, and propagates the other completion-signals sent by `s`, the program is
        ill-formed with no diagnostic required.

2. The name `execution::lazy_upon_done` denotes a customization point object. For some subexpressions `s` and `f`, let `S` be `decltype((s))`. If `S` does not satisfy `execution::sender`, `execution::lazy_upon_done` is ill-formed. Otherwise, the expression
    `execution::lazy_upon_done(s, f)` is expression-equivalent to:

    1. `tag_invoke(lazy_upon_done, get_completion_scheduler<set_value>(s), s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(lazy_upon_done, s, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, constructs a sender `s2`. When `s2` is connected with some receiver `out_r`, it:

        1. Constructs a receiver `r`:

            1. When `execution::set_value(r, args...)` is called, calls `execution::set_value(out_r, args...)`.

            2. When `execution::set_error(r, e)` is called, calls `execution::set_error(out_r, e)`.

            3. When `execution::set_done(r)` is called, calls `invoke(f)` and passes the result `v` to `execution::set_value(out_r, v)`. If any of these operations throws an exception, it catches it and calls `execution::set_error(out_r, current_exception())`.

        2. Calls `execution::connect(s, r)`, which results in an operation state `op_state2`. If this operation throws an exception, it catches it and calls `execution::set_error(out_r, current_exception())`.

        3. Returns an operation state `op_state` that contains `op_state2`. When `execution::start(op_state)` is called, calls `execution::start(op_state2)`.

    If the function selected above does not return a sender which invokes `f` when the `set_done` signal of `s` is called, passing the return value as the value to any connected receivers, and propagates the other completion-signals sent by `s`, the program is
        ill-formed with no diagnostic required.

#### Bulk adaptor <b>[execution.sender.adaptors.bulk]</b> #### {#spec-execution.sender.adaptors.bulk}

1. `execution::bulk` and `execution::lazy_bulk` are used to run a task repeatedly for every index in an index space.

2. The name `execution::bulk` denotes a customization point object. For some subexpressions `s`, `shape`, and `f`, let `S` be `decltype((s))`, `Shape` be `decltype((shape))`, and `F` be `decltype((f))`. If `S` does not satisfy `execution::sender` or `Shape` does not
    satisfy `integral`, `execution::bulk` is ill-formed. Otherwise, the expression `execution::bulk(s, shape, f)` is expression-equivalent to:

    1. `tag_invoke(bulk, get_completion_scheduler<set_value>(s), s, shape, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(bulk, s, shape, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, `lazy_bulk(s, shape, f)`.

2. The name `execution::lazy_bulk` denotes a customization point object. For some subexpressions `s`, `shape`, and `f`, let `S` be `decltype((s))`, `Shape` be `decltype((shape))`, and `F` be `decltype((f))`. If `S` does not satisfy `execution::sender` or `Shape`
    does not satisfy `integral`, `execution::bulk` is ill-formed. Otherwise, the expression `execution::bulk(s, shape, f)` is expression-equivalent to:

    1. `tag_invoke(bulk, get_completion_scheduler<set_value>(s), s, shape, f)`, if that expression is valid and its type satisfies `execution::sender`.

    2. Otherwise, `tag_invoke(bulk, s, shape, f)`, if that expression is valid and its type satisfies `execution::sender`.

    3. Otherwise, constructs a sender `s2`. When `s2` is connected with some receiver `out_r`, it:

        1. Constructs a receiver `r`:

            1. When `execution::set_value(r, args...)` is called, calls `f(i, args...)` for each `i` of type `Shape` from `0` to `shape`, then calls `execution::set_value(out_r, args...)`. If any of these operations throws an exception, it catches it and calls
                `execution::set_error(out_r, current_exception())`.

            2. When `execution:;set_error(r, e)` is called, calls `execution:;set_error(out_r, e)`.

            3. When `execution::set_done(r, e)` is called, calls `execution:;set_done(out_r, e)`.

        2. Calls `execution::connect(s, r)`, which results in an operation state `op_state2`. If this operation throws an exception, it catches it and calls `execution::set_error(out_r, current_exception())`.

        3. Returns an operation state `op_state` that contains `op_state2`. When `execution:;start(op_state)` is called, calls `execution::start(op_state2)`.

    If the function selected above does not return a sender which invokes `f(i, args...)` for each `i` of type `Shape` from `0` to `shape` when the predecessor sender sends values `args...`, or does not propagate the values of the signals sent by the predecessor to
        a connected receiver, the program is ill-formed with no diagnostic required.

#### When_all adaptor <b>[execution.sender.adaptors.when_all]</b> #### {#spec-execution.sender.adaptor.when_all}

1. `execution::when_all` and `execution::lazy_when_all` are used to join multiple sender chains and create a sender whose execution is dependent on all of the predecessors.

2. The name `execution::when_all` denotes a customization point object. For some subexpressions `s...`, let `S` be `decltype((s))`. If any type <code>S<i><sub>i</sub></i></code> in `S...` does not satisfy `execution::typed_sender`, `execution::when_all` is
    ill-formed. Otherwise, the expression `execution::when_all(s...)` is expression-equivalent to:

    1. `tag_invoke(when_all, s...)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected by `tag_invoke` does not return a sender which sends a concatenation of values sent by `s...` when they all complete with
        `set_value`, the program is ill-formed with no diagnostic required.

    2. Otherwise, `lazy_when_all(s...)`.

3. The name `execution::lazy_when_all` denotes a customization point object. For some subexpressions `s...`, let `S` be `decltype((s))`. If any type <code>S<i><sub>i</sub></i></code> in `S...` does not satisfy `execution::typed_sender`, `execution::lazy_when_all` is
    ill-formed. Otherwise, the expression `execution::lazy_when_all(s...)` is expression-equivalent to:

    1. `tag_invoke(lazy_when_all, s...)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected by `tag_invoke` does not return a sender which sends a concatenation of values sent by `s...` when they all complete with
        `set_value`, the program is ill-formed with no diagnostic required.

    2. Otherwise, constructs a sender `s`. When `s` is connected with some receiver `out_r`, it:

        1. For each sender <code>s<i><sub>i</sub></i></code> in `s...`, constructs a receiver <code>r<sub><i>i</i></sub></code>:

            2. If <code>execution::set_value(r<sub><i>i</i></sub>, t<sub><i>i</i></sub>...)</code> is called for every <code>r<sub><i>i</i></sub></code>,
                <code>execution::set_value(out_r, t<sub><i>0</i></sub>..., t<sub><i>1</i></sub>..., ..., t<sub><i>n</i></sub>...)</code> is called, where `n` is `sizeof...(s) - 1`.

            2. Otherwise, if <code>execution::set_error(r<sub><i>i</i></sub>, e)</code> is called for any <code>r<sub><i>i</i></sub></code>, `execution::set_error(out_r, e)` is called.

            3. Otherwise, if <code>execution::set_done(r<sub><i>i</i></sub>)</code> is called for any <code>r<sub><i>i</i></sub></code>, `execution::set_done(out_r)` is called.

        3. For each sender <code>s<i><sub>i</sub></i></code> in `s...`, calls <code>execution::connect(s<sub><i>i</i></sub>, r<sub><i>i</i></sub>)</code>, resulting in operation states <code>op_state<sub><i>i</i></sub></code>.

        4. Returns an operation state `op_state` that contains each operation state <code>op_state<sub><i>i</i></sub></code>. When `execution:;start(op_state)` is called, calls <code>execution::start(op_state<sub><i>i</i></sub>)</code> for each
            <code>op_state<sub><i>i</i></sub></code>.

4. Senders returned from `execution::when_all` and `execution::lazy_when_all` shall not expose the sender queries `get_completion_scheduler<CPO>`.

5. `tag_invoke` expressions used in the definitions of `execution::when_all` and `execution::lazy_when_all` shall not consider member functions of their first non-tag arguments.

#### Reschedule_when_all adaptor <b>[execution.sender.adaptors.reschedule_when_all]</b> #### {#spec-execution.sender.adaptor.reschedule_when_all}

1. `execution::when_all` and `execution::lazy_when_all` are used to join multiple sender chains and create a sender whose execution is dependent on all of the predecessors, while also making sure that they complete on the specified scheduler. [<i>Note:</i> this can
    allow for better customization of the algorithm. --<i>end note</i>]

2. The name `execution::reschedule_when_all` denotes a customization point object. For some subexpressions `sch` and `s...`, let `Sch` be `decltype(sch)` and `S` be `decltype((s))`. If `Sch` does not satisfy `scheduler`, or any type <code>S<i><sub>i</sub></i></code>
    in `S...` does not satisfy `execution::typed_sender`, `execution::reschedule_when_all` is ill-formed. Otherwise, the expression `execution::reschedule_when_all(sch, s...)` is expression-equivalent to:

    1. `tag_invoke(reschedule_when_all, sch, s...)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected by `tag_invoke` does not return a sender which sends a concatenation of values sent by `s...` when they all complete
        with `set_value`, or does not send its completion signals, other than ones resulting from a scheduling error, on an execution agent belonging to the associated execution context of `sch`, the program is ill-formed with no diagnostic required.

    2. Otherwise, `reschedule(when_all(s...), sch)`.

3. The name `execution::lazy_reschedule_when_all` denotes a customization point object. For some subexpressions `sch` and `s...`, let `Sch` be `decltype(sch)` and `S` be `decltype((s))`. If `Sch` does not satisfy `scheduler`, or any type
    <code>S<i><sub>i</sub></i></code> in `S...` does not satisfy `execution::typed_sender`, `execution::lazy_reschedule_when_all` is ill-formed. Otherwise, the expression `execution::lazy_reschedule_when_all(sch, s...)` is expression-equivalent to:

    1. `tag_invoke(lazy_reschedule_when_all, sch, s...)`, if that expression is valid and its type satisfies `execution::sender`. If the function selected by `tag_invoke` does not return a sender which sends a concatenation of values sent by `s...` when they all
        complete with `set_value`, or does not send its completion signals, other than ones resulting from a scheduling error, on an execution agent belonging to the associated execution context of `sch`, the program is ill-formed with no diagnostic required.

    2. Otherwise, `lazy_reschedule(lazy_when_all(s...), sch)`.

4. Senders returned from `execution::reschedule_when_all` and `execution::lazy_reschedule_when_all` shall not propagate the sender queries `get_completion_scheduler<CPO>` to predecessor senders. They shall return a scheduler equivalent to the `sch` argument from
    those queries.

### Sender algorithms <b>[execution.sender.algorithms]</b> ### {#spec-execution.sender.algorithms}

## One-way execution <b>[execution.execute]</b> ## {#spec-execution.execute}
